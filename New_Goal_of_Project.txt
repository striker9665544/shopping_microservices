Detailed Breakdown of New Complexities
1. Service Registry & Discovery (e.g., Eureka, Consul)
Problem It Solves: In the previous design, the API Gateway and search-service had hardcoded URLs like http://mobile-service:8082. What if you want to change the port or run multiple versions? A service registry automates this.
How It Works:
You deploy a Service Registry (like Netflix Eureka) as a pod in Kubernetes.
Every single microservice (mobile-service, api-gateway, etc.), upon starting, registers itself with the registry, saying "Hi, I'm mobile-service, and you can find me at this IP and port."
When the API Gateway needs to find the mobile-service, it asks the registry: "Where can I find an available instance of mobile-service?" The registry provides the location.
Kubernetes Complexity: This adds a new stateful component to your cluster. You'll need Deployment and Service YAMLs for the Eureka server itself.
Project Complexity: All your microservices need a new "Eureka Client" dependency and configuration to enable registration.
2. Centralized Configuration (e.g., Spring Cloud Config Server)
Problem It Solves: You have 10 microservices, each with its own application.properties. If the database password changes, you have to update and redeploy 10 services. This is unmanageable.
How It Works:
You deploy a Config Server pod.
You store all your configurations (database URLs, passwords, feature flags) in a central Git repository.
The Config Server reads from this Git repository.
All other microservices, on startup, connect to the Config Server to fetch their specific configuration.
Kubernetes Complexity: Another stateful service to deploy and manage.
Project Complexity: All services need the "Spring Cloud Config Client" dependency. You replace most of their application.properties with a simple bootstrap.properties that just points to the Config Server.
3. Asynchronous Communication (e.g., RabbitMQ, Kafka)
Problem It Solves: When a user places an order, you don't want them to wait for the system to process payment, update inventory, and send an email confirmation. That's slow and risky (what if the email service is down?).
How It Works:
You introduce a Message Broker (like RabbitMQ) into your cluster.
You create a new order-service. When a user clicks "Add to Cart" or "Checkout", the responsible service doesn't call other services directly. Instead, it publishes a message like "ProductAddedToCartEvent" or "OrderPlacedEvent" to a queue in RabbitMQ.
Other services (like inventory-service, notification-service) subscribe to these queues. When they see a new message, they process it independently and in the background.
Benefit: This decouples your services. The checkout process is now lightning-fast for the user. The system becomes highly resilient; if the notification service is down, the order is still placed, and the notification will be sent when the service comes back online.
Kubernetes Complexity: Deploying a message broker is a significant step, often requiring a StatefulSet to manage its persistent data.
4. Circuit Breakers (e.g., Resilience4j)
Problem It Solves: Your search-service calls 5 other services. What if shoes-service is slow or down? The search-service will have threads stuck waiting, potentially causing it to crash too, leading to a "cascading failure" that takes down your whole site.
How It Works:
You wrap the inter-service calls (e.g., in the API Gateway or search-service) with a Circuit Breaker pattern.
Logic: "If I call shoes-service and it fails 5 times in a row, I'll 'trip the breaker.' For the next 30 seconds, I won't even try to call it. I'll immediately return a default response (like an empty list of shoes) or an error."
After 30 seconds, it will try one "half-open" call. If it succeeds, the breaker closes. If it fails, it stays open.
Project Complexity: Add the Resilience4j dependency to services making critical outbound calls and configure the circuit breaker behavior.
5. Observability Stack (Prometheus, Grafana, Jaeger)
Problem It Solves: With 10+ services, how do you know if everything is healthy? If a request is slow, where is the bottleneck?
How It Works:
Metrics (Prometheus): Each microservice exposes a /actuator/prometheus endpoint. Prometheus is a time-series database that periodically "scrapes" (reads) metrics like CPU usage, memory, and request latency from these endpoints.
Dashboards (Grafana): Grafana connects to Prometheus as a data source and allows you to build powerful, real-time dashboards to visualize the health and performance of your entire system on one screen.
Distributed Tracing (Jaeger): When a request enters the API Gateway, it's assigned a unique Trace ID. This ID is passed along to every single microservice that the request touches. All services send their "span" data (how long they took) to a Jaeger server. Jaeger then reconstructs the entire journey of the request, showing you a waterfall diagram of exactly where time was spent.
Kubernetes Complexity: This involves deploying Prometheus, Grafana, and Jaeger as separate applications within your cluster, each with its own configuration and storage requirements.